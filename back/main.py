from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import os
import tempfile
import whisper
import librosa
import soundfile as sf
import logging
import uvicorn
import numpy as np
import torch
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
from fastapi.staticfiles import StaticFiles
from starlette.exceptions import HTTPException as StarletteHTTPException
import uvicorn
import logging
# from routers import emotion, diary, feedback, statistics
# from config.database import db_manager
# from config.settings import settings
# from config.exceptions import (
#     validation_exception_handler,
#     http_exception_handler,
#     voice_diary_exception_handler,
#     general_exception_handler,
#     VoiceDiaryException
# )
# ë¡œê¹… ì„¤ì •
from fastapi import FastAPI
from pydantic import BaseModel
from PIL import Image, ImageDraw, ImageFont
import openai
import requests
import uuid
import io
import os
import textwrap
import time
import dotenv
from fastapi import UploadFile, File
from typing import List
from fastapi.responses import JSONResponse
from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
import firebase_admin
from firebase_admin import credentials, storage
cred = credentials.Certificate("diaryemo-5e11e-firebase-adminsdk-fbsvc-3960bbf582.json")

firebase_admin.initialize_app(cred, {
    'storageBucket': 'diaryemo-5e11e.firebasestorage.app'
})

bucket = storage.bucket()
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

whisper_model = None

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    try:
        await db_manager.connect_to_database()
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ (í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰): {e}")
    
    # OpenAI API í”¼ë“œë°± ìƒì„±ê¸° ì´ˆê¸°í™” (Mock ë²„ì „ì—ì„œëŠ” ì£¼ì„ ì²˜ë¦¬)
    # try:
    #     await feedback_generator.load_model()
    #     logger.info("OpenAI API í”¼ë“œë°± ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    # except Exception as e:
    #     logger.error(f"OpenAI API í”¼ë“œë°± ìƒì„±ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    #     # í”¼ë“œë°± ìƒì„±ê¸° ì—†ì´ë„ ì„œë²„ ì‹œì‘ ê°€ëŠ¥ (fallback ì‚¬ìš©)
    #     logger.info("fallback í”¼ë“œë°± ìƒì„±ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ ì‹œì‘")
    logger.info("Mock ì„œë¹„ìŠ¤ë¡œ ì„œë²„ ì‹œì‘")
    
    logger.info("ìŒì„± ì¸ì‹ ì¼ê¸° ë°±ì—”ë“œ ì„œë²„ ì‹œì‘ ì™„ë£Œ")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_whisper_model():
    """OpenAI Whisper ëª¨ë¸ ë¡œë“œ (íŒ¨ë”© ë¬¸ì œ ì—†ìŒ)"""
    global whisper_model
    try:
        logger.info("OpenAI Whisper ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
        
        # Whisper ëª¨ë¸ ë¡œë“œ (tiny ëª¨ë¸ - ë¹ ë¥´ê³  ì•ˆì •ì )
        whisper_model = whisper.load_model("tiny", device=device)
        
        logger.info("OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return True
    except Exception as e:
        logger.error(f"Whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def preprocess_audio_simple(audio_data):
    """ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬"""
    try:
        # ì„ì‹œ íŒŒì¼ì— ì˜¤ë””ì˜¤ ë°ì´í„° ì €ì¥
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_data)
            temp_file_path = temp_file.name
        
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
            audio, sr = librosa.load(temp_file_path, sr=16000)
            
            # ê¸°ë³¸ ì •ë³´ ë¡œê¹…
            duration = len(audio) / sr
            logger.info(f"ì˜¤ë””ì˜¤ ê¸¸ì´: {duration:.2f}ì´ˆ")
            
            # ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ ì²˜ë¦¬
            if duration < 0.1:
                logger.warning("ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. 0.5ì´ˆë¡œ íŒ¨ë”©í•©ë‹ˆë‹¤.")
                target_length = int(0.5 * sr)
                audio = np.pad(audio, (0, max(0, target_length - len(audio))), 'constant')
            
            # ë„ˆë¬´ ê¸´ ì˜¤ë””ì˜¤ ì²˜ë¦¬ (30ì´ˆë¡œ ì œí•œ)
            if duration > 30:
                audio = audio[:30*sr]
                logger.info("ì˜¤ë””ì˜¤ë¥¼ 30ì´ˆë¡œ ìë¦„")
            
            # ë³¼ë¥¨ ì •ê·œí™”
            if np.max(np.abs(audio)) > 0:
                audio = audio / np.max(np.abs(audio)) * 0.95
            
            # ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì €ì¥
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as processed_temp:
                sf.write(processed_temp.name, audio, sr)
                processed_path = processed_temp.name
            
            # ì›ë³¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(temp_file_path)
            
            logger.info(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(audio)/sr:.2f}ì´ˆ")
            return processed_path
            
        except Exception as preprocessing_error:
            logger.error(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {preprocessing_error}")
            return temp_file_path
            
    except Exception as e:
        logger.error(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def transcribe_audio(audio_file_path):
    """OpenAI Whisperë¡œ ìŒì„± ì¸ì‹ (íŒ¨ë”© ë¬¸ì œ ì—†ìŒ)"""
    try:
        if whisper_model is None:
            return "Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(audio_file_path)
        logger.info(f"ìŒì„± ì¸ì‹í•  íŒŒì¼ í¬ê¸°: {file_size} bytes")
        
        if file_size < 1000:
            return "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ë” ê¸¸ê²Œ ë…¹ìŒí•´ì£¼ì„¸ìš”."
        
        if file_size > 25 * 1024 * 1024:  # 25MB ì œí•œ
            return "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. ë” ì§§ê²Œ ë…¹ìŒí•´ì£¼ì„¸ìš”."
        
        # OpenAI Whisperë¡œ ìŒì„± ì¸ì‹
        logger.info("OpenAI Whisperë¡œ ìŒì„± ì¸ì‹ ì‹œì‘...")
        
        # ì˜µì…˜ ì„¤ì •
        options = {
            "language": "ko",  # í•œêµ­ì–´ ì„¤ì •
            "task": "transcribe",
            "fp16": torch.cuda.is_available(),  # GPU ì‚¬ìš© ì‹œ fp16 í™œì„±í™”
            "no_speech_threshold": 0.6,
            "logprob_threshold": -1.0,
            "condition_on_previous_text": False,  # ì´ì „ í…ìŠ¤íŠ¸ ì¡°ê±´ ë¹„í™œì„±í™”
            "initial_prompt": None,
            "word_timestamps": False
        }
        
        # ìŒì„± ì¸ì‹ ì‹¤í–‰
        result = whisper_model.transcribe(audio_file_path, **options)
        
        # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = result.get("text", "").strip()
        
        if text:
            logger.info(f"ìŒì„± ì¸ì‹ ì„±ê³µ: {text}")
            return text
        else:
            logger.warning("ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return "ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ëª…í™•í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”."
            
    except Exception as e:
        logger.error(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
        return "ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

# @app.get("/")
# async def root():
#     """ë©”ì¸ í˜ì´ì§€"""
#     return {
#         'message': 'STT ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤!',
#         'model': 'OpenAI Whisper (íŒ¨ë”© ë¬¸ì œ ì™„ì „ í•´ê²°)',
#         'version': '2.0.0',
#         'status': 'ready' if whisper_model else 'loading',
#         'docs': '/docs'
#     }

# @app.get("/health")
# async def health_check():
#     """ì„œë²„ ìƒíƒœ í™•ì¸"""
#     return {
#         'status': 'healthy',
#         'model_loaded': whisper_model is not None,
#         'device': 'cuda' if torch.cuda.is_available() else 'cpu'
#     }

# @app.post("/api/speech-to-text")
# async def convert_speech(audio: UploadFile = File(...)):
#     """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” API (OpenAI Whisper ì‚¬ìš©)"""
#     try:
#         # ëª¨ë¸ ë¡œë“œ í™•ì¸
#         if whisper_model is None:
#             raise HTTPException(
#                 status_code=503, 
#                 detail="Whisper ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
#             )
        
#         # íŒŒì¼ ê²€ì¦
#         if not audio.filename:
#             raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
#         logger.info(f"ìŒì„± íŒŒì¼ ìˆ˜ì‹ : {audio.filename}")
        
#         # ì˜¤ë””ì˜¤ ë°ì´í„° ì½ê¸°
#         audio_data = await audio.read()
        
#         # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
#         processed_audio_path = preprocess_audio_simple(audio_data)
#         if processed_audio_path is None:
#             raise HTTPException(status_code=400, detail="ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
        
#         try:
#             # ìŒì„± ì¸ì‹ ì‹¤í–‰
#             text = transcribe_audio(processed_audio_path)
            
#             return {
#                 'success': True,
#                 'text': text,
#                 'message': 'ìŒì„± ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'
#             }
            
#         finally:
#             # ì„ì‹œ íŒŒì¼ ì •ë¦¬
#             if os.path.exists(processed_audio_path):
#                 os.unlink(processed_audio_path)
            
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"API ì˜¤ë¥˜: {e}")
#         return JSONResponse(
#             status_code=500,
#             content={
#                 'success': False,
#                 'text': '',
#                 'message': f'ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
#             }
#         )

# ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    logger.info("=" * 50)
    logger.info("STT ì„œë²„ ì‹œì‘ ì¤‘...")
    logger.info("OpenAI Whisper ì‚¬ìš© (íŒ¨ë”© ë¬¸ì œ í•´ê²°)")
    logger.info("=" * 50)
    
    success = load_whisper_model()
    if not success:
        logger.error("âŒ Whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!")
        logger.error("ì„œë²„ëŠ” ì‹œì‘ë˜ì§€ë§Œ STT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        logger.info("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        logger.info("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")
    
    logger.info("=" * 50)
    logger.info("ì„œë²„ ì£¼ì†Œ: http://localhost:8000")
    logger.info("API ì—”ë“œí¬ì¸íŠ¸: /api/speech-to-text")
    logger.info("API ë¬¸ì„œ: http://localhost:8000/docs")
    logger.info("=" * 50)

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000)
























# AI ëª¨ë¸ ì„í¬íŠ¸ (Mock ë²„ì „)
# from services.feedback_generator import feedback_generator

# ë¡œê¹… ì„¤ì •
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
# )
# logger = logging.getLogger(__name__)

# app = FastAPI(
#     title="ìŒì„± ì¸ì‹ ì¼ê¸° ë°±ì—”ë“œ API",
#     description="""
#     ## ìŒì„± ì¸ì‹ ê¸°ë°˜ ì¼ê¸° ì„œë¹„ìŠ¤ ğŸ¤ğŸ“–
    
#     í•œêµ­ì–´ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì‘ì„±í•œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ì • ë¶„ì„ê³¼ AI í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì¼ê¸° ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
    
#     ### ì£¼ìš” ê¸°ëŠ¥
#     - ğŸ¤– **KoBERT/KoELECTRA ê°ì • ë¶„ì„**: 7ê°€ì§€ ê°ì • ë¶„ë¥˜ (ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë‘ë ¤ì›€, ë†€ëŒ, í˜ì˜¤, ì¤‘ì„±)
#     - ğŸ’¬ **KoGPT ê³µê° í”¼ë“œë°±**: ê³µê°í˜•, ê²©ë ¤í˜•, ë¶„ì„í˜• 3ê°€ì§€ ìŠ¤íƒ€ì¼
#     - ğŸ“Š **ê°ì • í†µê³„ ë¶„ì„**: ì¼/ì£¼/ì›”/ë…„ ë‹¨ìœ„ ê°ì • ì¶”ì´ ë° ì¸ì‚¬ì´íŠ¸
#     - ğŸ“ **ì¼ê¸° ê´€ë¦¬**: CRUD ê¸°ëŠ¥ ë° ìë™ ê°ì • ë¶„ì„/í”¼ë“œë°± ì—°ë™
    
#     ### API ì—”ë“œí¬ì¸íŠ¸
#     - `/api/v1/emotion/`: ê°ì • ë¶„ì„ ê´€ë ¨ API
#     - `/api/v1/feedback/`: í”¼ë“œë°± ìƒì„± ê´€ë ¨ API  
#     - `/api/v1/diary/`: ì¼ê¸° ê´€ë¦¬ ê´€ë ¨ API
#     - `/api/v1/statistics/`: ê°ì • í†µê³„ ê´€ë ¨ API
#     """,
#     version="1.0.0",
#     docs_url="/docs",
#     redoc_url="/redoc",
#     openapi_tags=[
#         {
#             "name": "ê°ì •ë¶„ì„",
#             "description": "KoBERT/KoELECTRA ëª¨ë¸ì„ í™œìš©í•œ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"
#         },
#         {
#             "name": "í”¼ë“œë°±ìƒì„±", 
#             "description": "KoGPT ëª¨ë¸ì„ í™œìš©í•œ ê³µê° í”¼ë“œë°± ìƒì„±"
#         },
#         {
#             "name": "ì¼ê¸°ì²˜ë¦¬",
#             "description": "Firebase ì €ì¥ëœ ì¼ê¸° ë°ì´í„° ê°ì • ë¶„ì„/í”¼ë“œë°± ì²˜ë¦¬"
#         },
#         {
#             "name": "í†µê³„ë¶„ì„",
#             "description": "ê°ì • í†µê³„ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ì œê³µ"
#         }
#     ]
# )



# @app.get("/")
# async def root():
#     """ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
#     return {
#         "message": "ìŒì„± ì¸ì‹ ì¼ê¸° ë°±ì—”ë“œ ì„œë²„ê°€ ì •ìƒ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤.",
#         "status": "running",
#         "version": "1.0.0",
#         "test_page": "http://localhost:8000/static/index.html",
#         "api_docs": "http://localhost:8000/docs"
#     }

# @app.get("/health")
# async def health_check():
#     """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
#     return {"status": "healthy"}

# # ì˜ˆì™¸ ì²˜ë¦¬ê¸° ë“±ë¡
# app.add_exception_handler(RequestValidationError, validation_exception_handler)
# app.add_exception_handler(StarletteHTTPException, http_exception_handler)
# app.add_exception_handler(VoiceDiaryException, voice_diary_exception_handler)
# app.add_exception_handler(Exception, general_exception_handler)

# @app.on_event("shutdown")
# async def shutdown_event():
#     """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
#     try:
#         await db_manager.close_database_connection()
#         logger.info("ìŒì„± ì¸ì‹ ì¼ê¸° ë°±ì—”ë“œ ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")
#     except Exception as e:
#         logger.error(f"ì„œë²„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

# # Static íŒŒì¼ ì„œë¹™ (í…ŒìŠ¤íŠ¸ í˜ì´ì§€ìš©)
# app.mount("/static", StaticFiles(directory="static"), name="static")

# # ë¼ìš°í„° ë“±ë¡
# app.include_router(emotion.router, prefix="/api/v1/emotion", tags=["ê°ì •ë¶„ì„"])
# app.include_router(feedback.router, prefix="/api/v1/feedback", tags=["í”¼ë“œë°±ìƒì„±"])
# app.include_router(diary.router, prefix="/api/v1/diary", tags=["ì¼ê¸°ì²˜ë¦¬"])
# app.include_router(statistics.router, prefix="/api/v1/statistics", tags=["í†µê³„ë¶„ì„"])

# # if __name__ == "__main__":
# #     uvicorn.run(
# #         "main:app",
# #         host="0.0.0.0",
# #         port=8001,
# #         reload=True
# #     ) 




class DiaryRequest(BaseModel):
    diary_text: str
    font_path: str = "NanumGothic.ttf"  # ê¸°ë³¸ í°íŠ¸ ê²½ë¡œ
    user_name: str = "ë‚˜"
    gender: str = "ì—¬ì„±"  # ê¸°ë³¸ ì„±ë³„
def get_script(user_name:str,diary: str) -> list:
    CHARACTER_STYLES = {
    "female": {
        "default": "A kind-looking Korean woman in her 20s with straight black hair, wearing casual indoor clothes.",
    },
    "male": {
        "default": "A calm Korean man in his late 20s with short black hair and glasses, wearing a hoodie.",
    },
}
    prompt = f"""
    
You are a Japanese comic writer INOUE TAKEHIKO. The following diary entry is written by {user_name}.
Do not use speech bubbles.
First, translate the diary entry to fluent English if it's not already in English.
Then, generate a 4-panel wholesome slice-of-life comic scenario only with {CHARACTER_STYLES}.

Each panel must include a 'Scene' and a 'Dialogue' in the following format:

[Panel 1]
Scene: ...
Dialogue: ...

[Panel 2]
...


Diary: {diary}
"""

    res = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
    )
    text = res.choices[0].message.content.strip()
    print("\n========== GPT RESPONSE START ==========")
    print(text)
    print("=========== GPT RESPONSE END ===========\n")

    scenes = []
    for i in range(1, 5):
        start_token = f"[Panel {i}]"
        end_token = f"[Panel {i+1}]" if i < 4 else None

        if start_token not in text:
            raise ValueError(f"{start_token} not found in GPT output.")

        part = text.split(start_token)[1]
        if end_token and end_token in part:
            part = part.split(end_token)[0]

        lines = part.strip().splitlines()
        scene = {"scene": "", "dialogue": ""}
        for line in lines:
            if line.lower().startswith("scene:"):
                scene["scene"] = line.split(":", 1)[-1].strip()
            elif line.lower().startswith("dialogue:"):
                scene["dialogue"] = line.split(":", 1)[-1].strip()
        scenes.append(scene)
    
    return scenes

def translate_text_to_korean(text: str) -> str:
    res = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": f"Translate this into comics style natural Korean:\n{text}"}
        ],
        temperature=0.5,
    )
    return res.choices[0].message.content.strip()

# def build_combined_prompt(scenes: list) -> str:
    
#     prompt = "Create a 4-panel wholesome slice-of-life comic. Each panel is described below:\n"
#     for i, scene in enumerate(scenes, 1):
#         prompt += f"[Panel {i}] Scene: {scene['scene']}. Dialogue: {scene['dialogue']}\n"
#     prompt += (
#         "Draw all 4 panels in a single 1024x1024 image, arranged in 2x2 layout. "
#         "Do not include any text for image generation. Asian style art, and consistent korean characters. "
#         "Avoid violence, sensitive topics, or anything that violates content policies."
#     )
#     return prompt



def build_combined_prompt(scenes: list, user_name: str, gender: str) -> str:
    CHARACTER_STYLES = {
    "female": {
        "default": "A sexy-looking Japanese woman in her 20s with straight black hair.",
    },
    "male": {
        "default": "A handsome-looking South Korean man in his 20s with short black hair.",
    },
}
    character_desc = CHARACTER_STYLES.get(gender, {}).get("default", "")
    prompt = (
        f"The main character is {user_name}. "
        f"{character_desc} "
        "Do not use speech bubbles.Create a 4-panel wholesome slice-of-life comic. Each panel is described below:\n"
    )
    for i, scene in enumerate(scenes, 1):
        prompt += f"[Panel {i}] Scene: {scene['scene']}. Dialogue: {scene['dialogue']}\n"
    prompt += (
        "Draw all 4 panels in a single 1024x1024 image, arranged in 2x2 layout. "
        "Do not include any text for image generation. Asian style art, and consistent Korean characters. "
        "Avoid violence, sensitive topics, or anything that violates content policies."
    )
    return prompt


def generate_combined_image(prompt: str, max_retries: int = 3) -> Image.Image:
    for attempt in range(1, max_retries + 1):
        try:
            response = client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                size="1024x1024",
                quality="standard",
                n=1,
                response_format="url",
                style="natural",
            )
            image_url = response.data[0].url
            image_data = requests.get(image_url).content
            return Image.open(io.BytesIO(image_data))
        except openai.BadRequestError as e:
            if "content_policy_violation" in str(e):
                print(f"ğŸš« Policy violation (attempt {attempt}/{max_retries})")
                if attempt == max_retries:
                    raise RuntimeError("Content policy violation â€“ all retries failed.")
                time.sleep(1)
            else:
                raise

# def add_text_boxes_to_combined_image(
#     img: Image.Image,
#     scenes: list,
#     font_path: str = None,
#     *,
#     font_size: int = 28,
#     box_height_ratio: float = 0.22,
#     padding: int = 20,
#     text_color: str = "black",
#     box_fill: str = "white",
#     box_outline: str = "black",
#     max_chars_per_line: int = 17
# ) -> Image.Image:
#     draw = ImageDraw.Draw(img)
#     font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()

#     panel_width = img.width // 2
#     panel_height = img.height // 2
#     box_height = int(panel_height * box_height_ratio)

#     positions = [
#         (0, 0), (panel_width, 0),
#         (0, panel_height), (panel_width, panel_height)
#     ]

#     for idx, (x, y) in enumerate(positions):
#         original_dialogue = scenes[idx]['dialogue']
#         translated = translate_text_to_korean(original_dialogue)
#         wrapped = textwrap.fill(translated, width=max_chars_per_line)
#         lines = wrapped.split("\n")

#         box_top = y + panel_height - box_height
#         box_bottom = y + panel_height
#         draw.rectangle(
#             [x, box_top, x + panel_width, box_bottom],
#             fill=box_fill,
#             outline=box_outline
#         )

#         text_y = box_top + padding
#         for line in lines:
#             draw.text((x + padding, text_y), line, fill=text_color, font=font)
#             text_y += font_size + 6

#     return img

def add_text_boxes_to_combined_image(
    img: Image.Image,
    scenes: list,
    font_path: str = None,
    *,
    base_font_size: int = 28,
    min_font_size: int = 14,
    box_height_ratio: float = 0.22,
    padding: int = 20,
    text_color: str = "black",
    box_fill: str = "white",
    box_outline: str = "white",
    max_chars_per_line: int = 20,
    max_lines: int = 5
) -> Image.Image:
    draw = ImageDraw.Draw(img)
    panel_width = img.width // 2
    panel_height = img.height // 2
    box_height = int(panel_height * box_height_ratio)

    positions = [
        (0, 0), (panel_width, 0),
        (0, panel_height), (panel_width, panel_height)
    ]

    for idx, (x, y) in enumerate(positions):
        original_dialogue = scenes[idx]['dialogue']
        translated = translate_text_to_korean(original_dialogue)

        font_size = base_font_size
        while font_size >= min_font_size:
            font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()
            wrapped = textwrap.fill(translated, width=max_chars_per_line)
            lines = wrapped.split('\n')

            total_text_height = len(lines) * (font_size + 6) + padding * 2
            if total_text_height <= box_height:
                break
            font_size -= 2

        if font_size < min_font_size:
            font_size = min_font_size
            font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()
            wrapped = textwrap.fill(translated, width=max_chars_per_line)
            lines = wrapped.split('\n')
            if len(lines) > max_lines:
                lines = lines[:max_lines]
                lines[-1] = lines[-1].rstrip() + "..."
        else:
            font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()

        box_top = y + panel_height - box_height
        box_bottom = y + panel_height
        draw.rectangle(
            [x, box_top, x + panel_width, box_bottom],
            fill=box_fill,
            outline=box_outline
        )

        text_y = box_top + padding
        for line in lines:
            draw.text((x + padding, text_y), line, fill=text_color, font=font)
            text_y += font_size + 6

    return img


# @app.post("/generate-comic")
# def generate_comic(req: DiaryRequest):
    
#     scenes = get_script(req.user_name, req.diary_text)
#     prompt = build_combined_prompt(scenes, req.user_name, req.gender)
#     comic_img = generate_combined_image(prompt)
#     comic_img = add_text_boxes_to_combined_image(comic_img, scenes, font_path="Danjo-bold-Regular.otf")

#     filename = f"comic_{uuid.uuid4().hex[:8]}.png"
#     output_dir = "outputs"
#     os.makedirs(output_dir, exist_ok=True)
#     output_path = os.path.join(output_dir, filename)
#     comic_img.save(output_path)

#     return {
#     "filename": filename,
#     "url": f"http://localhost:8080/outputs/{filename}",
#     "original_text": req.diary_text
# }

import uuid

@app.post("/generate-comic")
def generate_comic(req: DiaryRequest):
    scenes = get_script(req.user_name, req.diary_text)
    prompt = build_combined_prompt(scenes, req.user_name, req.gender)
    comic_img = generate_combined_image(prompt)
    comic_img = add_text_boxes_to_combined_image(comic_img, scenes, font_path="Danjo-bold-Regular.otf")

    filename = f"comic_{uuid.uuid4().hex[:8]}.png"
    output_dir = "outputs"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)
    comic_img.save(output_path)

    temp_local_path = output_path
    blob = bucket.blob(f"comics/{filename}")

    blob.upload_from_filename(temp_local_path)
    blob.make_public()
    public_url = blob.public_url

    return {
        "filename": filename,
        "url": public_url,
        "original_text": req.diary_text
    }

@app.post("/api/speech-to-text")
async def speech_to_diary(audio: UploadFile = File(...)):
    """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ChatGPTë¡œ ì¼ê¸° í˜•ì‹ ì •ë¦¬"""
    try:
        # Whisper ëª¨ë¸ í™•ì¸
        if whisper_model is None:
            raise HTTPException(
                status_code=503,
                detail="Whisper ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
        
        if not audio.filename:
            raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logger.info(f"ìŒì„± íŒŒì¼ ìˆ˜ì‹ : {audio.filename}")
        audio_data = await audio.read()

        # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        processed_audio_path = preprocess_audio_simple(audio_data)
        if processed_audio_path is None:
            raise HTTPException(status_code=400, detail="ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨")

        try:
            # 1. Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
            raw_text = transcribe_audio(processed_audio_path)

            # 2. ChatGPTë¡œ ì¼ê¸° í˜•íƒœë¡œ ì •ë¦¬
            diary_text = generate_diary_text(raw_text)

            return {
                'success': True,
                # 'raw_text': raw_text,
                'text': diary_text,
                'message': 'ìŒì„±ì´ ì¼ê¸° í˜•ì‹ìœ¼ë¡œ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.'
            }

        finally:
            if os.path.exists(processed_audio_path):
                os.unlink(processed_audio_path)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"API ì˜¤ë¥˜: {e}")
        return JSONResponse(
            status_code=500,
            content={
                'success': False,
                
                'text': '',
                'message': f'ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
            }
        )
def generate_diary_text(text: str) -> str:
    prompt = f"""
    ì‚¬ìš©ìê°€ ëŒ€ì¶© ìŒì„±ìœ¼ë¡œ ë…¹ìŒí•´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ëœ ë‚´ìš©ì´ì—ìš”. ì¼ê¸°ë¡œ ë§Œë“¤ë©´ ë˜ìš”. ë”í•˜ê±°ë‚˜ ëœì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë§Œë“œì„¸ìš”. ë‚´ìš© ì „ì²´ë¥¼ í‘œí˜„í•˜ëŠ” ê°ì • ì´ëª¨ì§€ë„ ì „ë‹¬í•˜ì„¸ìš”.
    "{text}"
    
    ì¼ê¸° í˜•ì‹ìœ¼ë¡œ ì •ë¦¬ëœ ê¸€:
    """

    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìì˜ í•˜ë£¨ë¥¼ ì •ë¦¬í•´ì£¼ëŠ” ì¼ê¸° ì‘ê°€ì•¼."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=600
    )

    return response.choices[0].message.content